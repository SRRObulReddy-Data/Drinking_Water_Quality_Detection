## Data Splitting:
In machine learning in order to access the performance of the classifier one need to train the classifier using training set and then test the performance of the classifier on unseen testing set. The whole dataset is generally divided into two parts i.e. training set and testing set. In this project the data set has been divided in the ratio of 8:2. Eighty percent of the data set is used for training, while the remaining twenty percent is used for testing. 

## Support Vector Machine: 

### What is SVM? What are it's advantages and disadvantages?
They are just a group of supervised learning methods for classification, regression, and 
detecting outliers. 

The advantages of SVM are: 
  1. Only accurate when the number of samples are less than the number of dimensions. 
  2. It uses a subset of coaching points (called support vectors) within the decision function, 
      making it memory efficient. 
  3. Versatile: Different Kernel functions can be specified for the option function. Custom 
      kernels may be defined in addition to the standard kernels.
     
The disadvantages of SVM are: 
  1. If the number of features exceeds the number of samples, over fitting in chosen kernel 
      function will be prevented. 
  2. Probability estimates are determined using a rich five-fold cross-validation method 
      rather than directly by SVMs.

## Random Forest Algorithm:

  1. A Random Forest is a technique that performs both regression and classification tasks by combining multiple decision trees and a technique known as Bootstrap and Aggregation, also known as bagging. Rather         than relying on individual decision trees, the basic concept is to combine several decision trees to determine the final production. 
  2. It's a form of learning in which you combine different algorithms or use the same algorithm multiple times to create a more efficient prediction model.
  3. The Random Forest algorithm constructs decision trees from data samples, and then receives predictions from each one before voting on the best solution.
  4. Since it combines the outcomes to reduce over-fitting, it's a better ensemble solution than a single decision tree.

